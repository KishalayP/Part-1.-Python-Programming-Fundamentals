{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "import os\n",
    "plaidml.keras.install_backend()\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'plaidml.keras.backend' from 'e:\\\\microsoft visual studio\\\\shared\\\\python37_64\\\\lib\\\\site-packages\\\\plaidml\\\\keras\\\\backend.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as k\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TILE program construction utilities.\n",
    "\n",
    "When writing a program in TILE, you typically specify a graph of operations,\n",
    "where each operation is defined by a single TILE function.  You then use\n",
    "composition to connect the individual TILE functions into a single composite\n",
    "TILE function, which can then be compiled, scheduled, and executed as a single\n",
    "unit.  The PlaidML API provides functions for building up and using these\n",
    "composite TILE programs.\n",
    "\n",
    "Defining the individual per-operation TILE functions is sometimes trivial,\n",
    "and sometimes not.  For example: although TILE makes it very easy to write\n",
    "a matrix multiply operation, the details of how that operations is expressed\n",
    "in TILE vary depending on the number of dimensions involved, whether\n",
    "broadcasting is required, &c.  Higher-level frameworks tend to expect their\n",
    "backends to have a single \"Do a matrix multiply\" operation that's supposed\n",
    "to internally figure out all of these details; that's not really something\n",
    "that can be done in TILE directly.\n",
    "\n",
    "It's wasteful and error-prone to implement these sometimes-tricky conversions\n",
    "from high-level semantics to low-level TILE code for each framework.  And\n",
    "the frameworks tend to have similar semantics, thanks to the influence of\n",
    "Numpy.  So PlaidML provides:\n",
    "\n",
    "  * A standard high-level operation library for constructing the per-operation\n",
    "    TILE functions and composing them together (this module)\n",
    "\n",
    "  * Utilities to assist in constructing operations, such as broadcasting logic\n",
    "    (this module)\n",
    "\n",
    "  * A suite of operations that we've found to be useful across a variety of\n",
    "    frameworks (the :doc:`plaidml.op` module).\n",
    "\n",
    "This library uses two passes for building up composite functions.  The first\n",
    "pass constructs Python objects representing the operation graph; the second\n",
    "pass translates the operation graph to the composite TILE function.  This is\n",
    "done for two reasons: it allows for higher-level optimizations (e.g.\n",
    "translating particular subtrees to more efficient TILE operations) and for\n",
    "expressing operations that cannot be efficiently implemented in the current\n",
    "version of TILE (e.g. it's very expensive to implement ArgMax in the initial\n",
    "released version of TILE, but ArgMax is typically used in composite\n",
    "expressions like ``Equal(ArgMax(X), ArgMax(Y))``, which is trivial to\n",
    "efficiently implement in TILE).\n",
    "\n",
    "More precisely, this library builds up a bipartite directed acyclic graph of\n",
    "``Operation`` and ``Value`` objects.  ``Operation`` is the base class of each\n",
    "operation; ``Value`` represents an operation input or output.  ``compose``\n",
    "translates an operation graph into a ``plaidml.Function``.\n",
    "\n",
    "See the `Tile Tutorial <https://github.com/plaidml/plaidml/wiki/Tile-Tutorial>`_\n",
    "for more information about how the TILE language works, or check out the\n",
    "`PlaidML Op Tutorial <https://github.com/plaidml/plaidml/wiki/PlaidML-Op-Tutorial>`_\n",
    "for the details of writing your own operations.\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "\n",
    "import plaidml\n",
    "import six\n",
    "import sys\n",
    "\n",
    "\n",
    "class Error(Exception):\n",
    "    \"\"\"Errors raised during TILE function composition.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class LogicError(Error):\n",
    "    \"\"\"Logic errors on the part of the caller.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "NUMPY_DTYPE_TO_PLAIDML = {\n",
    "    'bool': plaidml.DType.BOOLEAN,\n",
    "    'float16': plaidml.DType.FLOAT16,\n",
    "    'float32': plaidml.DType.FLOAT32,\n",
    "    'float64': plaidml.DType.FLOAT64,\n",
    "    'int8': plaidml.DType.INT8,\n",
    "    'int16': plaidml.DType.INT16,\n",
    "    'int32': plaidml.DType.INT32,\n",
    "    'int64': plaidml.DType.INT64,\n",
    "    'uint8': plaidml.DType.UINT8,\n",
    "    'uint16': plaidml.DType.UINT16,\n",
    "    'uint32': plaidml.DType.UINT32,\n",
    "    'uint64': plaidml.DType.UINT64,\n",
    "}\n",
    "\n",
    "PLAIDML_DTYPE_TO_NUMPY = dict([[v, k] for k, v in NUMPY_DTYPE_TO_PLAIDML.items()])\n",
    "\n",
    "Source = namedtuple('Source', ['op', 'output_name'])\n",
    "\n",
    "\n",
    "class Shape(namedtuple('Shape', ['dtype', 'dims'])):\n",
    "    \"\"\"Represents a symbolic tensor shape.\"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    @property\n",
    "    def ndims(self):\n",
    "        \"\"\"The shape's dimensionality.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of dimensions in the shape.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.dims)\n",
    "\n",
    "\n",
    "class _OpBindings(object):\n",
    "    \"\"\"A mapping of bindings discovered during an operation binding traversal.\"\"\"\n",
    "\n",
    "    def __init__(self, ctx, dev):\n",
    "        \"\"\"Initialize the `_OpBindings`.\n",
    "\n",
    "        Args:\n",
    "            ctx (plaidml.Context): The context for the binding traversal.\n",
    "        \"\"\"\n",
    "        self.ctx = ctx\n",
    "        self.dev = dev\n",
    "        self._bindings = dict()\n",
    "\n",
    "    def lookup(self, operation):\n",
    "        \"\"\"Looks up an operation's output bindings within this traversal.\n",
    "\n",
    "        Args:\n",
    "            operation (Operation): The operation producing the outputs.\n",
    "\n",
    "        Returns:\n",
    "            str -> plaidml.Var: The operation's bound outputs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._bindings[operation]\n",
    "        except KeyError:\n",
    "            six.raise_from(\n",
    "                LogicError('Unbound operation encountered during operation composition'), None)\n",
    "\n",
    "    def insert(self, operation, outputs):\n",
    "        \"\"\"Adds an operation's output bindings.\n",
    "\n",
    "        Args:\n",
    "            operation (Operation): The operation that's been bound.\n",
    "            outputs (str -> plaidml.Var): The operation's output bindings.\n",
    "        \"\"\"\n",
    "        self._bindings[operation] = outputs\n",
    "\n",
    "    def is_bound(self, operation):\n",
    "        \"\"\"Indicates whether an operation's already been bound.\n",
    "\n",
    "        Args:\n",
    "            operation (Operation): The operation to look up.\n",
    "\n",
    "        Returns:\n",
    "            bool: True iff the operation's already been bound.\n",
    "        \"\"\"\n",
    "        return operation in self._bindings\n",
    "\n",
    "\n",
    "class Operation(object):\n",
    "    \"\"\"Operation base class.\"\"\"\n",
    "\n",
    "    def __init__(self, code, inputs, outputs, name=None, side_effects=None):\n",
    "        \"\"\"`Operation` constructor.\n",
    "\n",
    "        Most `Operation`s are defined via subclasses whose constructors\n",
    "        build up the TILE code to implement the requested operation and\n",
    "        then invoke this constructor to initialize the underlying\n",
    "        `Operation` functionality.  This makes it simpler for binding-time\n",
    "        code generation to recognize and transform chunks of the operation\n",
    "        graph (since they can just check that operations are subclasses\n",
    "        of known derived classes).\n",
    "\n",
    "        Operations may have side-effects -- tensors that are to be updated\n",
    "        as a side-effect of evaluating the operation.  These are supplied as\n",
    "        a list of `(variable, new_value)` tuples, where both `variable` and\n",
    "        `new_value` are `Value` objects.  These will be wrapped up in the\n",
    "        function returned by `compose()`.\n",
    "\n",
    "        Args:\n",
    "            code (str): The code implementing the operation, or None.\n",
    "            inputs ([(str, Value)]): Operation inputs.\n",
    "            outputs ([(str, Shape)]): Operation outputs.\n",
    "            name (str): A name for this operation, or None.\n",
    "            side_effects ([Value, Value]): A dict of side-effects of this operation.\n",
    "        \"\"\"\n",
    "        self.code = code\n",
    "        self.inputs = dict([(k, _ShapelessValue.from_value(Value.from_python_value(v)))\n",
    "                            for k, v in inputs])\n",
    "        output_list = [(output_name, Value.for_op(shape, self, output_name))\n",
    "                       for output_name, shape in outputs]\n",
    "        self.output_tuple = tuple([val for _, val in output_list])\n",
    "        self.outputs = dict(output_list)\n",
    "        self.name = name or self.__class__.__name__\n",
    "        self.side_effects = side_effects or []\n",
    "\n",
    "    def sole_output(self):\n",
    "        if len(self.output_tuple) != 1:\n",
    "            raise LogicError(\n",
    "                'Sole output requested from operation {}; operation has multiple outputs ({})' \\\n",
    "                .format(self.name, ', '.join(self.outputs.keys())))\n",
    "        return self.output_tuple[0]\n",
    "\n",
    "    @classmethod\n",
    "    def function(cls, *args, **kwargs):\n",
    "        \"\"\"Invokes an `Operation` as a function.\n",
    "\n",
    "        When processing the operation graph, it's useful for each `Operation`\n",
    "        type to be its own class, enabling operation-specific behaviors.  But\n",
    "        it's awkward to use a class as a function.  This classmethod\n",
    "        instantiates an `Operation` subclass using the supplied inputs, and\n",
    "        returns the operation's outputs as a tuple of `Value`s (or as a single\n",
    "        `Value` for single-output operations).\n",
    "\n",
    "        Args:\n",
    "            args: The operation constructor positional arguments.\n",
    "            kwargs: The operation constructor keyword arguments.\n",
    "\n",
    "        Raises:\n",
    "            LogicError: If invoked on Operation instead of on an Operation subclass.\n",
    "\n",
    "        Returns:\n",
    "            tuple(Value): The operation outputs, in operation-defined order.\n",
    "        \"\"\"\n",
    "        if cls is Operation:\n",
    "            raise LogicError(\n",
    "                'Operation.function is defined on subclasses of Operation, not Operation itself.')\n",
    "        operation = cls(*args, **kwargs)\n",
    "        if len(operation.output_tuple) == 1:\n",
    "            return operation.output_tuple[0]\n",
    "        return operation.output_tuple\n",
    "\n",
    "    def bind(self, bindings):\n",
    "        \"\"\"Builds an output variable dictionary for the operation.\n",
    "\n",
    "        N.B. Subclasses may override this method in order to implement optimizations and\n",
    "        composite operations that aren't easily described in TILE.\n",
    "\n",
    "        Args:\n",
    "            bindings (_OpBindings): The previously-computed output bindings.\n",
    "\n",
    "        Returns:\n",
    "            str -> plaidml.Var: The bound outputs for this operation.  The caller is responsible\n",
    "                                for adding this to the known output bindings; this is typically\n",
    "                                called by _OpBindings.__missing__, which does this automatically.\n",
    "        \"\"\"\n",
    "        if not self.code:\n",
    "            raise NotImplementedError('{} is not directly implemented.'.format(\n",
    "                self.__class__.__name__))\n",
    "        applier = plaidml.Applier(bindings.ctx, plaidml.Function(self.code))\n",
    "        for input_name, input_value in self.inputs.items():\n",
    "            applier.add_input(input_name, input_value.bind(bindings))\n",
    "        outputs = {}\n",
    "        for output_name in self.outputs.keys():\n",
    "            try:\n",
    "                outputs[output_name] = applier.add_output(output_name)\n",
    "            except BaseException as e:\n",
    "                raise Exception('Failed to add output \\'{}\\' in op {}: {}; code={}'.format(\n",
    "                    output_name, self.name, e.message, self.code))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def unary_op(value, op_str, name=None):\n",
    "    \"\"\"Builds a Value for an elementwise unary operation.\n",
    "\n",
    "    Args:\n",
    "        value (Value): The operation input.\n",
    "        op_str (str): The string to use for the operation.\n",
    "                      The string should be an expression in terms of 'I'.\n",
    "        name (str): The name of the operation, or None.\n",
    "\n",
    "    Returns:\n",
    "        Value: A Value representing the result of the operation.\n",
    "    \"\"\"\n",
    "    operation = Operation(\n",
    "        'function (I) -> (O) {{ O = {}; }}'.format(op_str), [('I', value)], [('O', value.shape)],\n",
    "        name=name)\n",
    "\n",
    "    return operation.sole_output()\n",
    "\n",
    "\n",
    "def binary_op(lhs, rhs, op_str, dtype=None, name=None):\n",
    "    \"\"\"Builds a Value for an elementwise binary operation.\n",
    "\n",
    "    Args:\n",
    "        lhs (Value or numeric): The left-hand side of the operation.\n",
    "        rhs (Value or numeric): The right-hand side of the operation.\n",
    "        op_str (str): The string to use for the operation.\n",
    "                      The string should be an expression in terms of 'L' and 'R'.\n",
    "        dtype (plaidml.DType): If not None, supplies the operation dtype;\n",
    "                               otherwise, the common dtype of lhs and rhs is used.\n",
    "        name (str): The name of the operation, or None.\n",
    "\n",
    "    Returns:\n",
    "        Value: A Value representing the result of the operation.\n",
    "    \"\"\"\n",
    "    lhs = Value.from_python_value(lhs)\n",
    "    rhs = Value.from_python_value(rhs)\n",
    "\n",
    "    shape = Shape(\n",
    "        common_dtype(lhs.shape.dtype, rhs.shape.dtype),\n",
    "        broadcast_dims(lhs.shape.dims, rhs.shape.dims))\n",
    "\n",
    "    if dtype:\n",
    "        shape = Shape(dtype, shape.dims)\n",
    "\n",
    "    operation = Operation(\n",
    "        'function (L, R) -> (O) {{ O = {}; }}'.format(op_str), [('L', lhs), ('R', rhs)],\n",
    "        [('O', shape)],\n",
    "        name=name)\n",
    "\n",
    "    return operation.sole_output()\n",
    "\n",
    "\n",
    "def maximum(x, y):\n",
    "    if isinstance(x, Value) or isinstance(y, Value):\n",
    "        return binary_op(x, y, 'max(L, R)', name='Maximum')\n",
    "    else:\n",
    "\n",
    "        return max(x, y)\n",
    "\n",
    "\n",
    "def minimum(x, y):\n",
    "    if isinstance(x, Value) or isinstance(y, Value):\n",
    "        return binary_op(x, y, 'min(L, R)', name='Minimum')\n",
    "    else:\n",
    "\n",
    "        return min(x, y)\n",
    "\n",
    "\n",
    "class ShapeOf(Operation):\n",
    "    \"\"\"\n",
    "    Computes the shape of a supplied tensor.\n",
    "\n",
    "    (N.B. This is in tile.py instead of in op.py solely because it's useful\n",
    "          to be able to reference it from Value.__getitem__().  For future-proofing,\n",
    "          frameworks should reference this class as op.ShapeOf.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.source = x\n",
    "        super(ShapeOf, self).__init__('function (I) -> (O) { O = shape(I); }', [('I', x)],\n",
    "\n",
    "                                      [('O', Shape(plaidml.DType.INT32, (x.shape.ndims,)))])\n",
    "\n",
    "\n",
    "shape_of = ShapeOf.function\n",
    "\n",
    "\n",
    "class _SliceOf(Operation):\n",
    "    \"\"\"\n",
    "    Computes a subslice of a supplied tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value, key):\n",
    "\n",
    "        if isinstance(key, slice) or isinstance(key, int) or isinstance(key, type(Ellipsis)):\n",
    "            key = (key,)\n",
    "        if not isinstance(key, tuple):\n",
    "            raise ValueError('Cannot index Values using type {}'.format(type(key)))\n",
    "        if key.count(Ellipsis) > 1:\n",
    "            raise ValueError('Cannot use multiple ellipses in a slice (given {})'.format(key))\n",
    "\n",
    "        var_list = list()\n",
    "        dim_list = list()\n",
    "        formula_list = list()\n",
    "        offset_list = list()\n",
    "        dims = list()\n",
    "        inner_idx = 0\n",
    "        extra_vars = []\n",
    "        try:\n",
    "            ellipsis_idx = key.index(Ellipsis)\n",
    "        except ValueError:\n",
    "            ellipsis_idx = None\n",
    "        if ellipsis_idx is not None:\n",
    "            ellipsis_length = value.shape.ndims - len(key) + 1\n",
    "            if ellipsis_length < 0:\n",
    "                raise ValueError('Slice key too long. Tensor has {} dimensions, key is {}'.format(\n",
    "                    value.shape.ndims, key))\n",
    "            key = tuple(\n",
    "                list(key[:ellipsis_idx]) + [slice(None, None, None)] * ellipsis_length + list(\n",
    "                    key[ellipsis_idx + 1:]))\n",
    "        for idx in range(len(key)):\n",
    "            length_numerator, length_numerator_value, step, offset, idx_extra_vars = self._parse_slice(\n",
    "                value.shape.dims, key, idx)\n",
    "            extra_vars.extend(idx_extra_vars)\n",
    "            if step == None:\n",
    "                # In this case offset is an int\n",
    "                if offset >= 0:\n",
    "                    formula_list.append('{}'.format(offset))\n",
    "                else:\n",
    "                    offset_list.append('Offset{} = N{}+{};'.format(idx, idx, offset))\n",
    "                    formula_list.append('{}'.format('Offset{}'.format(idx)))\n",
    "            else:\n",
    "                var_list.append('i{}'.format(inner_idx))\n",
    "                dim_subs = {'numer': length_numerator, 'step': step}\n",
    "                if step > 0:\n",
    "                    dim_list.append('({numer} + {step} - 1)/{step}'.format(**dim_subs))\n",
    "                else:\n",
    "                    dim_list.append('({numer} + {step} + 1)/{step}'.format(**dim_subs))\n",
    "                if isinstance(length_numerator, str):\n",
    "                    dims.append(unary_op(length_numerator_value / step, 'ceil(I)', 'Ceiling'))\n",
    "                    offset_list.append('Offset{} = {};'.format(idx, offset))\n",
    "                    formula_list.append('{}*i{}+{}'.format(step, inner_idx,\n",
    "                                                           'Offset{}'.format(idx)))\n",
    "                else:\n",
    "                    dims.append(int(math.ceil(float(length_numerator) / step)))\n",
    "                    formula_list.append('{}*i{}+{}'.format(step, inner_idx, offset))\n",
    "                inner_idx += 1\n",
    "\n",
    "        # Separately handle extra indices not sliced over\n",
    "        for idx in range(len(key), value.shape.ndims):\n",
    "            var_list.append('i{}'.format(inner_idx))\n",
    "            dim_list.append('N{}'.format(idx))\n",
    "            dims.append(value.shape.dims[idx])\n",
    "            formula_list.append('i{}'.format(inner_idx))\n",
    "            inner_idx += 1\n",
    "        dims = tuple(dims)\n",
    "\n",
    "        if len(dims) == 0:\n",
    "            body = 'O[] = =(I[' + ', '.join(formula_list) + ']);'\n",
    "        else:\n",
    "            body = 'O[{}: {}] = =(I[{}]);'.format(', '.join(var_list), ', '.join(dim_list),\n",
    "                                                  ', '.join(formula_list))\n",
    "\n",
    "        # TODO: Example below is out of date, although it shows the spirit of the op\n",
    "        # Example 'code' (slicing X[5:10,3,:,2:6:2]):\n",
    "        #   function (I[N0, N1, N2, N3]) -> (O) {\n",
    "        #     O[i0, i1, i2: 5, N2, 2] = +(I[i0+5, 3, i1, 2*i2+2]);\n",
    "        #   }\n",
    "        prefix = '\\n                   '\n",
    "        code = \"\"\"\n",
    "               function (I[{indims}]) -> (O) {{\n",
    "                   {extra_vars}{offsets}{body}\n",
    "               }}\"\"\".format(\n",
    "            indims=', '.join(['N{}'.format(i) for i in range(value.shape.ndims)]),\n",
    "            extra_vars=''.join(v + prefix for v in extra_vars),\n",
    "            offsets=''.join(o + prefix for o in offset_list),\n",
    "            body=body)\n",
    "\n",
    "        super(_SliceOf, self).__init__(\n",
    "            code, [('I', value)], [('O', Shape(value.shape.dtype, dims))], name='SliceOf')\n",
    "        self.key = key\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_slice(dims, key, idx):\n",
    "        extra_vars = []\n",
    "        if isinstance(key[idx], int):\n",
    "            return 1, 1, None, key[idx], extra_vars\n",
    "\n",
    "        def check(val):\n",
    "            if isinstance(val, int):\n",
    "                return\n",
    "            if val is None:\n",
    "                return\n",
    "            raise ValueError('Must use ints when slicing; received {}'.format(val))\n",
    "\n",
    "        check(key[idx].start)\n",
    "        check(key[idx].stop)\n",
    "        check(key[idx].step)\n",
    "\n",
    "        step = 1 if key[idx].step is None else key[idx].step\n",
    "        if step == 0:\n",
    "            raise ValueError('Cannot slice with step size 0')\n",
    "\n",
    "        start = key[idx].start\n",
    "        if start == None:\n",
    "            if step > 0:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = -1\n",
    "\n",
    "        start_value = start\n",
    "\n",
    "        if start < 0:\n",
    "            if isinstance(dims[idx], Value):\n",
    "                start = 'N{} + {}'.format(idx, start)\n",
    "            else:\n",
    "                start = dims[idx] + start\n",
    "            start_value = dims[idx] + start_value\n",
    "\n",
    "        if step > 0:\n",
    "            if isinstance(dims[idx], Value):\n",
    "                extra_vars.append('Start{idx} = max({start}, 0);'.format(start=start, idx=idx))\n",
    "                start = 'Start{}'.format(idx)\n",
    "            else:\n",
    "                start = maximum(start, 0)\n",
    "            start_value = maximum(start_value, 0)\n",
    "        else:\n",
    "            if isinstance(dims[idx], Value):\n",
    "                extra_vars.append('Start{idx} = min({start}, N{idx} - 1);'.format(\n",
    "                    start=start, idx=idx))\n",
    "                start = 'Start{}'.format(idx)\n",
    "            else:\n",
    "                start = minimum(start, dims[idx] - 1)\n",
    "            start_value = minimum(start_value, dims[idx] - 1)\n",
    "\n",
    "        stop = key[idx].stop\n",
    "        if stop == None:\n",
    "            if step > 0:\n",
    "                if isinstance(dims[idx], Value):\n",
    "                    stop = 'N{}'.format(idx)\n",
    "                else:\n",
    "                    stop = dims[idx]\n",
    "                stop_value = dims[idx]\n",
    "            else:\n",
    "                stop = -1\n",
    "                stop_value = -1\n",
    "            # Can return now and skip unneeded max/min\n",
    "            if isinstance(dims[idx], Value):\n",
    "                return '({} - ({}))'.format(\n",
    "                    stop, start), stop_value - start_value, step, start, extra_vars\n",
    "            return stop - start, stop_value - start_value, step, start, extra_vars\n",
    "\n",
    "        stop_value = stop\n",
    "\n",
    "        if stop < 0:\n",
    "            if isinstance(dims[idx], Value):\n",
    "                stop = 'N{} + {}'.format(idx, stop)\n",
    "            else:\n",
    "                stop = dims[idx] + stop\n",
    "            stop_value = dims[idx] + stop_value\n",
    "\n",
    "        if step > 0:\n",
    "            if isinstance(dims[idx], Value):\n",
    "                extra_vars.append('Stop{idx} = min({stop}, N{idx});'.format(stop=stop, idx=idx))\n",
    "                stop = 'Stop{}'.format(idx)\n",
    "            else:\n",
    "                stop = minimum(stop, dims[idx])\n",
    "            stop_value = minimum(stop_value, dims[idx])\n",
    "        else:\n",
    "            if isinstance(dims[idx], Value):\n",
    "                extra_vars.append('Stop{idx} = max({stop}, -1);'.format(stop=stop, idx=idx))\n",
    "                stop = 'Stop{}'.format(idx)\n",
    "            else:\n",
    "                stop = maximum(stop, -1)\n",
    "            stop_value = maximum(stop_value, -1)\n",
    "\n",
    "        if isinstance(dims[idx], Value):\n",
    "            length_numerator = '({} - ({}))'.format(stop, start)\n",
    "        else:\n",
    "            length_numerator = stop - start\n",
    "        return length_numerator, stop_value - start_value, step, start, extra_vars\n",
    "\n",
    "\n",
    "class _NDArray(Operation):\n",
    "    \"\"\"\n",
    "    An operation that builds a value from a Numpy ndarray.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value):\n",
    "        # TODO: Consider copying the value if it's writeable.\n",
    "        self._value = value\n",
    "        shape = Shape(NUMPY_DTYPE_TO_PLAIDML[value.dtype.name], tuple(value.shape))\n",
    "        super(_NDArray, self).__init__(None, [], [('O', shape)], name='NDArray')\n",
    "\n",
    "    def bind(self, bindings):\n",
    "        tensor = plaidml.Tensor(bindings.dev,\n",
    "                                plaidml.Shape(bindings.ctx,\n",
    "                                              NUMPY_DTYPE_TO_PLAIDML[self._value.dtype.name],\n",
    "                                              *self._value.shape))\n",
    "        with tensor.mmap_discard(bindings.ctx) as view:\n",
    "            view.copy_from_ndarray(self._value)\n",
    "            view.writeback()\n",
    "\n",
    "        return {'O': tensor}\n",
    "\n",
    "\n",
    "class _ShapelessValue(object):\n",
    "    \"\"\"\n",
    "    Wraps a PlaidML variable, without shape information.\n",
    "\n",
    "    Symbolic shape information requires a reference back to the underlying shaped\n",
    "    variable.  To support this without creating cycles in the operation graph,\n",
    "    `Operation` objects reference `_ShapelessValue` instances as their inputs.\n",
    "    `Value` then inherits from `_ShapelessValue` in order to augment the value\n",
    "    with shape information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var, source, name=None):\n",
    "        if (var is None) and (not source):\n",
    "            raise LogicError('Either a variable or a variable source must be supplied')\n",
    "        self.var = var\n",
    "        self.source = source\n",
    "        self._name = name\n",
    "\n",
    "    @staticmethod\n",
    "    def from_value(value):\n",
    "        return _ShapelessValue(value.var, value.source, value._name)\n",
    "\n",
    "    def __str__(self):\n",
    "        vcls = ' ' + self.var.__class__.__name__ if self.var else ''\n",
    "        return '{}{}'.format(self.name, vcls)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<tile._ShapelessValue {}>'.format(self)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        if self._name:\n",
    "            return self._name\n",
    "        if self.source:\n",
    "            if len(self.source.op.outputs) == 1:\n",
    "                return self.source.op.name\n",
    "            return self.source.op.name + '/' + self.source.output_name\n",
    "        return '<unnamed>'\n",
    "\n",
    "    @property\n",
    "    def key(self):\n",
    "        \"\"\"A dictionary key that unifies a Value with its shapeless slices.\"\"\"\n",
    "        return (self.var, self.source)\n",
    "\n",
    "    def bind(self, bindings):\n",
    "        \"\"\"Translates the `Value` to a PlaidML variable.\n",
    "\n",
    "        Args:\n",
    "            bindings (_OpBindings): The previously-computed output bindings.\n",
    "\n",
    "        Returns:\n",
    "            plaidml.Var: The variable representing this `Value`.\n",
    "        \"\"\"\n",
    "        if self.var:\n",
    "            return self.var\n",
    "        outputs = bindings.lookup(self.source.op)\n",
    "        return outputs[self.source.output_name]\n",
    "\n",
    "    def is_bound(self, bindings):\n",
    "        \"\"\"Indicates whether the `Value` has been bound.\n",
    "\n",
    "        Args:\n",
    "            bindings (_OpBindings): The bindings to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True iff the `Value` is a concrete PlaidML variable, or if its source operation\n",
    "                  is already bound in `bindings`.\n",
    "        \"\"\"\n",
    "        return self.var or (self.source and bindings.is_bound(self.source.op))\n",
    "\n",
    "\n",
    "class Value(_ShapelessValue):\n",
    "    \"\"\"A PlaidML variable and associated metadata.\"\"\"\n",
    "\n",
    "    def __init__(self, shape, var, source, name=None):\n",
    "        \"\"\"Constructs a `Value`.\n",
    "\n",
    "        This isn't typically used directly; instead, use one of the `Value`\n",
    "        classmethods.\n",
    "\n",
    "        Args:\n",
    "            shape (Shape): The `Value`'s symbolic shape.\n",
    "            var (plaidml.Var): The PlaidML variable backing this `Value`, or None.\n",
    "            source (Source): The source of the `Value`, or None.\n",
    "            name (str): A mnemonic name for the `Value`, or None.\n",
    "        \"\"\"\n",
    "        if not isinstance(shape.dims, tuple):\n",
    "            shape = Shape(shape.dtype, tuple(shape.dims))\n",
    "        self.shape = shape\n",
    "        super(Value, self).__init__(var, source, name)\n",
    "\n",
    "    def __str__(self):\n",
    "        vcls = ' ' + self.var.__class__.__name__ if self.var else ''\n",
    "        return '{}{} {}{}'.format(self.name, vcls,\n",
    "                                  str(self.shape.dtype).split('.')[-1], self.shape.dims)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<tile.Value {}>'.format(self)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.source and isinstance(self.source.op, ShapeOf):\n",
    "            return self.source.op.source.shape.dims.__getitem__(key)\n",
    "\n",
    "        # Otherwise, we need to perform the slice as a TILE operation.\n",
    "        return _SliceOf.function(self, key)\n",
    "\n",
    "    @staticmethod\n",
    "    def for_op(shape, operation, output, name=None):\n",
    "        \"\"\"Builds an operation output Value.\n",
    "\n",
    "        Args:\n",
    "            shape (Shape): The symbolic shape of the operation output.\n",
    "            operation (Operation): The operation producing the output.\n",
    "            output (str): The name of the operation output.\n",
    "            name (str): A mnemonic name for the `Value`, or None.\n",
    "\n",
    "        Returns:\n",
    "            Value: The operation output.\n",
    "        \"\"\"\n",
    "        return Value(shape, None, Source(operation, output), name)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_ndims(ndims, dtype=plaidml.DType.FLOAT32, name=None):\n",
    "        \"\"\"Builds an N-dimensional placeholder Value.\n",
    "\n",
    "        The resulting `Value`'s shape will contain `Value` instances that will\n",
    "        be computed at binding time from the actual dimensions of the bound\n",
    "        tensor.\n",
    "\n",
    "        Args:\n",
    "            ndims (int): The number of dimensions.\n",
    "            dtype (plaidml.DType): The element datatype.\n",
    "            name (str): A mnemonic name for the `Value`, or None.\n",
    "\n",
    "        Returns:\n",
    "            Value: The placeholder value.\n",
    "        \"\"\"\n",
    "        return Value.from_var(plaidml.Placeholder(ndims), [None] * ndims, dtype, name)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dimensions(dimensions, dtype=plaidml.DType.FLOAT32, name=None):\n",
    "        \"\"\"Builds an N-dimensional placeholder Value from a list of dimension sizes.\n",
    "\n",
    "        `None` elements in the dimension list will be replaced by `Value` instances\n",
    "        that will be computed at binding time from the actual dimensions of the bound\n",
    "        tensor.\n",
    "\n",
    "        Args:\n",
    "            dimensions (tuple or list): The size of each dimension.\n",
    "            dtype (plaidml.DType): The element datatype.\n",
    "            name (str): A mnemonic name for the `Value`, or None.\n",
    "\n",
    "        Returns:\n",
    "            Value: The placeholder value.\n",
    "        \"\"\"\n",
    "        return Value.from_var(plaidml.Placeholder(len(dimensions)), dimensions, dtype, name)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_var(var, dimensions, dtype=plaidml.DType.FLOAT32, name=None):\n",
    "        \"\"\"Builds a Value from a PlaidML variable.\n",
    "\n",
    "        `None` elements in the dimension list will be replaced by `Value` instances\n",
    "        that will be computed at binding time from the actual dimensions of the bound\n",
    "        tensor.\n",
    "\n",
    "        Args:\n",
    "            var (plaidml.Var): The variable to be wrapped by the Value.\n",
    "            dimensions (tuple or list): The size of each dimension.\n",
    "            dtype (plaidml.DType): The element datatype.\n",
    "            name (str): A mnemonic name for the `Value`, or None.\n",
    "\n",
    "        Returns:\n",
    "            Value: The wrapped value.\n",
    "        \"\"\"\n",
    "        ndims = len(dimensions)\n",
    "\n",
    "        # Create the value with a temporary zero-dimensional shape, so that it can\n",
    "        # be supplied to Operation instances that calculate its dimensions.\n",
    "        val = Value(Shape(dtype, tuple()), var, None, name)\n",
    "\n",
    "        # Create the dimensions list.\n",
    "        dims = [val._filldim(ndims, idx, dim) for idx, dim in enumerate(dimensions)]\n",
    "\n",
    "        # Update the Value to have the new shape.\n",
    "        val.shape = Shape(dtype, tuple(dims))\n",
    "        return val\n",
    "\n",
    "    @staticmethod\n",
    "    def from_python_value(py_val, dtype=None, name=None, ctx=None, dev=None):\n",
    "        \"\"\"Builds a Value from a Python value.\n",
    "\n",
    "        Note: if the context and device are present, the returned value will always be a concrete\n",
    "        `Value` (wrapping a PlaidML variable, not an `Operation` output).  Otherwise, the returned\n",
    "        `Value` may be an `Operation` output.\n",
    "\n",
    "        Args:\n",
    "            var: A value of a standard Python type.\n",
    "            dtype (plaidml.DType): The element datatype, or None.\n",
    "            name (str): A mnemonic name for the `Value`, or None.\n",
    "            ctx (plaidml.context.Context): The context to use for the variable, or None.\n",
    "            dev (plaidml.Device): The device to use for the variable, or None.\n",
    "\n",
    "        Returns:\n",
    "            Value: The wrapped value.\n",
    "        \"\"\"\n",
    "        if isinstance(py_val, Value):\n",
    "            return py_val\n",
    "        elif isinstance(py_val, plaidml.Var):\n",
    "            return py_val\n",
    "        elif isinstance(py_val, six.integer_types):\n",
    "            if dtype is None:\n",
    "                dtype = plaidml.DType.INT32\n",
    "            return Value.from_var(plaidml.Integer(py_val), tuple(), dtype, name=name)\n",
    "        elif isinstance(py_val, float):\n",
    "            if dtype is None:\n",
    "                dtype = plaidml.DType.FLOAT32\n",
    "            return Value.from_var(plaidml.Real(py_val), tuple(), dtype, name=name)\n",
    "        elif hasattr(py_val, 'shape') and hasattr(py_val, 'dtype'):\n",
    "            # Assume it's an ndarray.\n",
    "            if ctx and dev:\n",
    "                # We have the device; we can return a value immediately.\n",
    "                tensor = plaidml.Tensor(\n",
    "                    dev,\n",
    "                    plaidml.Shape(ctx, NUMPY_DTYPE_TO_PLAIDML[py_val.dtype.name], *py_val.shape))\n",
    "                with tensor.mmap_discard(ctx) as view:\n",
    "                    view.copy_from_ndarray(py_val)\n",
    "                    view.writeback()\n",
    "                return Value.from_var(\n",
    "                    tensor,\n",
    "                    py_val.shape,\n",
    "                    NUMPY_DTYPE_TO_PLAIDML[py_val.dtype.name],\n",
    "                    name='NDArray')\n",
    "            # Otherwise, defer the value creation.\n",
    "            return _NDArray(py_val).sole_output()\n",
    "        else:\n",
    "            raise NotImplementedError('Unable to build a Value from a \\'{}\\' instance'.format(\n",
    "                py_val.__class__.__name__))\n",
    "\n",
    "    def _filldim(self, ndims, idx, dim):\n",
    "        if dim is not None:\n",
    "            return dim\n",
    "        return self._dim(ndims, idx)\n",
    "\n",
    "    def _dim(self, ndims, idx):\n",
    "        \"\"\"The symbolic size a dimension of the supplied variable.\n",
    "\n",
    "        Args:\n",
    "            ndims (int): The total number of dimensions.\n",
    "            idx (int): The 0-based index of the dimension to get.\n",
    "\n",
    "        Returns:\n",
    "            Value: The size of dimension `idx` of `var`.\n",
    "        \"\"\"\n",
    "        code = 'function (I[{dims}]) -> (O) {{ O = D{idx}; }}'.format(\n",
    "            dims=','.join(['D{}'.format(i) for i in range(ndims)]), idx=str(idx))\n",
    "        shape = Shape(plaidml.DType.UINT64, tuple())\n",
    "        operation = Operation(code, [('I', self)], [('O', shape)], name='SymbolicDim')\n",
    "        return operation.outputs['O']\n",
    "\n",
    "    # Python numeric type methods.  These allow Value objects to be used in\n",
    "    # ordinary expressions, returning derived Values.\n",
    "\n",
    "    # Logical operations\n",
    "    #\n",
    "    # N.B. We neither define __eq__ nor __ne__, because Value objects are compared for\n",
    "    #      equality and inequality in a number of contexts, such as \"value in some_list\".\n",
    "    #      So we use standard Python object definitions for equality/inequality; callers\n",
    "    #      that want TILE operations for these should use the operation library's\n",
    "    #      `equal()` and `not_equal()` functions.\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return binary_op(self, other, 'L >= R', dtype=plaidml.DType.BOOLEAN, name='Ge')\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return binary_op(self, other, 'L > R', dtype=plaidml.DType.BOOLEAN, name='Gt')\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return binary_op(self, other, 'L <= R', dtype=plaidml.DType.BOOLEAN, name='Le')\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return binary_op(self, other, 'L < R', dtype=plaidml.DType.BOOLEAN, name='Lt')\n",
    "\n",
    "    # Arithmetic operations\n",
    "\n",
    "    def __abs__(self):\n",
    "        return unary_op(self, 'abs(I)', 'Abs')\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 0:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 0.0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L + R', name='Add')\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 0:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 0.0:\n",
    "            return self\n",
    "        return binary_op(other, self, 'L + R', name='RevAdd')\n",
    "\n",
    "    def __and__(self, other):\n",
    "        return binary_op(self, other, 'L & R', name='And')\n",
    "\n",
    "    def __rand__(self, other):\n",
    "        return binary_op(other, self, 'L & R', name='RevAnd')\n",
    "\n",
    "    def __div__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 1:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 1.0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L / R', name='Div')\n",
    "\n",
    "    def __rdiv__(self, other):\n",
    "        return binary_op(other, self, 'L / R', name='RevDiv')\n",
    "\n",
    "    def __floordiv__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 1:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 1.0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'floor(L / R)', name='FloorDiv')\n",
    "\n",
    "    def __rfloordiv__(self, other):\n",
    "        return binary_op(other, self, 'floor(L / R)', name='RevFloorDiv')\n",
    "\n",
    "    def __invert__(self):\n",
    "        return unary_op(self, '~I', 'Invert')\n",
    "\n",
    "    def __lshift__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L << R', name='LShift')\n",
    "\n",
    "    def __rlshift__(self, other):\n",
    "        return binary_op(other, self, 'L << R', name='RevLShift')\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 1:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 1.0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L * R', name='Mul')\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 1:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 1.0:\n",
    "            return self\n",
    "        return binary_op(other, self, 'L * R', name='RevMul')\n",
    "\n",
    "    def __neg__(self):\n",
    "        return unary_op(self, '-I', 'Negate')\n",
    "\n",
    "    def __or__(self, other):\n",
    "        return binary_op(self, other, 'L | R', name='Or')\n",
    "\n",
    "    def __ror__(self, other):\n",
    "        return binary_op(other, self, 'L | R', name='RevOr')\n",
    "\n",
    "    def __pos__(self):\n",
    "        return unary_op(self, 'I', 'Identity')\n",
    "\n",
    "    def __rshift__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L >> R', name='RShift')\n",
    "\n",
    "    def __rrshift__(self, other):\n",
    "        return binary_op(other, self, 'L >> R', name='RevRShift')\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 0:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 0.0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L - R', name='Sub')\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 0:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 0.0:\n",
    "            return self\n",
    "        return binary_op(other, self, 'L - R', name='RevSub')\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, six.integer_types) and other == 1:\n",
    "            return self\n",
    "        if isinstance(other, float) and other == 1.0:\n",
    "            return self\n",
    "        return binary_op(self, other, 'L / R', name='TrueDiv')\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return binary_op(other, self, 'L / R', name='RevTrueDiv')\n",
    "\n",
    "    def __xor__(self, other):\n",
    "        return binary_op(self, other, 'L ^ R', name='Xor')\n",
    "\n",
    "    def __rxor__(self, other):\n",
    "\n",
    "        return binary_op(other, self, 'L ^ R', name='RevXor')\n",
    "\n",
    "\n",
    "def compose(ctx, dev, inputs, outputs, updates=None):\n",
    "    \"\"\"Builds a TILE Function that computes the indicated values.\n",
    "\n",
    "    Args:\n",
    "        ctx (plaidml.Context): The context to use for building the function.\n",
    "        dev (plaidml.Device): The device used to build the function (where constants will live)\n",
    "        inputs ([(name, Value)]): A list of named input placeholders.\n",
    "        outputs ([(name, Value)]): A list of named output values.\n",
    "        updates ([(original, updated)]): A list of updates to perform (side-effects).\n",
    "\n",
    "    Returns:\n",
    "        plaidml.Invoker: The composed TILE function.\n",
    "    \"\"\"\n",
    "    bindings = _OpBindings(ctx, dev)\n",
    "    to_be_bound = [val for _, val in outputs]\n",
    "    if updates is None:\n",
    "        updates = []\n",
    "    else:\n",
    "        for original, updated in updates:\n",
    "            to_be_bound.append(original)\n",
    "            to_be_bound.append(updated)\n",
    "    to_be_bound.extend((val for _, val in inputs))\n",
    "    while to_be_bound:\n",
    "        current = to_be_bound.pop()\n",
    "        if current.is_bound(bindings):\n",
    "            continue\n",
    "        op = current.source.op\n",
    "        vals = list(op.inputs.values())\n",
    "        for v, nv in op.side_effects:\n",
    "            vals.append(v)\n",
    "            vals.append(nv)\n",
    "        if any(not isinstance(val, _ShapelessValue) for val in vals):\n",
    "            raise LogicError('Operation {} bound a non-Value; inputs={}, side_effects={}'.format(\n",
    "                op.name, op.inputs, op.side_effects))\n",
    "        reqs = [v for v in vals if not v.is_bound(bindings)]\n",
    "        if reqs:\n",
    "            to_be_bound.append(current)\n",
    "            to_be_bound.extend(reqs)\n",
    "            continue\n",
    "        bindings.insert(op, op.bind(bindings))\n",
    "        updates.extend(op.side_effects)\n",
    "\n",
    "    try:\n",
    "        composer = plaidml.Composer()\n",
    "        for (input_name, val) in inputs:\n",
    "            composer.add_input(input_name, val.bind(bindings))\n",
    "        for (output_name, val) in outputs:\n",
    "            binding = val.bind(bindings)\n",
    "            composer.add_output(output_name, binding)\n",
    "        for (original, updated) in updates:\n",
    "            composer.add_update(original.bind(bindings), updated.bind(bindings))\n",
    "\n",
    "        return composer.build()\n",
    "\n",
    "    except:\n",
    "        sys.stderr.writelines(to_dot(inputs, outputs, updates))\n",
    "\n",
    "        raise\n",
    "\n",
    "\n",
    "def to_dot(inputs, outputs, updates=None, name='Tile'):\n",
    "    \"\"\"Translates a chain of tensor computations to a DOT graph.\n",
    "\n",
    "    Args:\n",
    "        inputs ([(name, Value)]): Provides names for computation inputs.\n",
    "        outputs ([(name, Value)]): The outputs to use for deriving the graph.\n",
    "        updates ([(original, updates)]): Side-effect updates that are part of the computations.\n",
    "        name (str): The name to use for the graph, or None.\n",
    "\n",
    "    Yields:\n",
    "        The strings comprising the lines of the DOT graph.\n",
    "    \"\"\"\n",
    "    yield 'digraph {} {{\\n'.format(name)\n",
    "\n",
    "    # The general idea:\n",
    "    #   We're building a bipartite digraph.\n",
    "    #   Each Value becomes a Box graph node containings its type info.\n",
    "    #   Each Operation becomes an Oval containing its name\n",
    "    #   Each connection between Operations and Values is an edge, labeled with\n",
    "    #     the Value's name relative to the Operation.\n",
    "    #   We start by building a mapping from the initial graph inputs to their names,\n",
    "    #     adding all of the output values to a set, and initializing an object-to-name\n",
    "    #     map to the empty set.\n",
    "    #   At each step, we remove a value from the set:\n",
    "    #     If its producing operation has a name, we can continue.\n",
    "    #     Otherwise, we add:\n",
    "    #       A node for the operation\n",
    "    #       Nodes for all unnamed input and output values (adding them to the queue),\n",
    "    #       Graph edges for all of the operation inputs and outputs.\n",
    "    #   When the set is empty, we're done.\n",
    "    #\n",
    "    #   Since order of values doesn't really matter, we just use a list to maintain the set.\n",
    "\n",
    "    def value_label(val):\n",
    "        return re.escape(str(val))\n",
    "\n",
    "    def op_label(op):\n",
    "        return re.escape(op.name)\n",
    "\n",
    "    to_be_processed = []\n",
    "    names = {}\n",
    "\n",
    "    def name_generator():\n",
    "        next_idx = 1\n",
    "        while True:\n",
    "            yield 'n' + str(next_idx)\n",
    "            next_idx += 1\n",
    "\n",
    "    namegen = name_generator()\n",
    "\n",
    "    for name, val in inputs:\n",
    "        dot_name = next(namegen)\n",
    "        names[val.key] = dot_name\n",
    "        yield '  {} [label=\"{}\\\\n{}\" shape=circle];\\n'.format(dot_name, name, value_label(val))\n",
    "\n",
    "    for name, val in outputs:\n",
    "        dot_name = next(namegen)\n",
    "        names[val.key] = dot_name\n",
    "        yield '  {} [label=\"{}\\\\n{}\" shape=doublecircle];\\n'.format(dot_name, name,\n",
    "                                                                    value_label(val))\n",
    "        to_be_processed.append(val)\n",
    "\n",
    "    if updates:\n",
    "        for original, update in updates:\n",
    "            original_dot_name = next(namegen)\n",
    "            names[original.key] = original_dot_name\n",
    "            yield '  {} [label=\"{}\" shape=circle];\\n'.format(original_dot_name,\n",
    "                                                             value_label(original))\n",
    "            to_be_processed.append(original)\n",
    "\n",
    "            update_dot_name = next(namegen)\n",
    "            names[update.key] = update_dot_name\n",
    "            yield '  {} [label=\"{}\" shape=circle];\\n'.format(update_dot_name, value_label(update))\n",
    "            to_be_processed.append(update)\n",
    "\n",
    "            yield '  {} -> {} [style=dotted];\\n'.format(update_dot_name, original_dot_name)\n",
    "\n",
    "    while to_be_processed:\n",
    "        val = to_be_processed.pop()\n",
    "        if not val.source:\n",
    "            continue\n",
    "        op = val.source.op\n",
    "        if op in names:\n",
    "            continue\n",
    "        op_dot_name = next(namegen)\n",
    "        names[op] = op_dot_name\n",
    "        yield '  {} [label=\"{}\" shape=oval];\\n'.format(op_dot_name, op_label(op))\n",
    "        for name, inval in op.inputs.items():\n",
    "            if inval.key not in names:\n",
    "                dot_name = next(namegen)\n",
    "                names[inval.key] = dot_name\n",
    "                to_be_processed.append(inval)\n",
    "                yield '  {} [label=\"{}\" shape=box];\\n'.format(dot_name, value_label(inval))\n",
    "            yield '  {} -> {} [label=\"{}\"];\\n'.format(names[inval.key], op_dot_name, name)\n",
    "        for name, outval in op.outputs.items():\n",
    "            if outval.key not in names:\n",
    "                dot_name = next(namegen)\n",
    "                names[outval.key] = dot_name\n",
    "                to_be_processed.append(outval)\n",
    "                yield '  {} [label=\"{}\" shape=box];\\n'.format(dot_name, value_label(outval))\n",
    "            yield '  {} -> {} [label=\"{}\"];\\n'.format(op_dot_name, names[outval.key], name)\n",
    "\n",
    "    yield '}\\n'\n",
    "\n",
    "\n",
    "class DTypeInfo(namedtuple('DTypeInfo', ['base', 'width'])):\n",
    "    \"\"\"Describes a PlaidML datatype.\"\"\"\n",
    "    __slots__ = ()\n",
    "\n",
    "    @property\n",
    "    def bitwidth(self):\n",
    "        \"\"\"The number of bits occupied by an instance of the type.\"\"\"\n",
    "        if self.base == 'bool':\n",
    "            return 1\n",
    "\n",
    "        return self.width * 8\n",
    "\n",
    "\n",
    "DTYPE_INFOS = {\n",
    "    plaidml.DType.BOOLEAN: DTypeInfo(base='bool', width=1),\n",
    "    plaidml.DType.INT8: DTypeInfo(base='int', width=1),\n",
    "    plaidml.DType.INT16: DTypeInfo(base='int', width=2),\n",
    "    plaidml.DType.INT32: DTypeInfo(base='int', width=4),\n",
    "    plaidml.DType.INT64: DTypeInfo(base='int', width=8),\n",
    "    plaidml.DType.UINT8: DTypeInfo(base='uint', width=1),\n",
    "    plaidml.DType.UINT16: DTypeInfo(base='uint', width=2),\n",
    "    plaidml.DType.UINT32: DTypeInfo(base='uint', width=4),\n",
    "    plaidml.DType.UINT64: DTypeInfo(base='uint', width=8),\n",
    "    plaidml.DType.FLOAT16: DTypeInfo(base='float', width=2),\n",
    "    plaidml.DType.FLOAT32: DTypeInfo(base='float', width=4),\n",
    "    plaidml.DType.FLOAT64: DTypeInfo(base='float', width=8),\n",
    "}\n",
    "\n",
    "INFO_DTYPES = dict([[v, k] for k, v in DTYPE_INFOS.items()])\n",
    "\n",
    "\n",
    "def common_dtype(*args):\n",
    "    \"\"\"Finds the common dtype of a set of dtypes.\n",
    "\n",
    "    Args:\n",
    "        args ([plaidml.DType]): The list of dtypes to be considered.\n",
    "\n",
    "    Returns:\n",
    "        plaidml.DType: The smallest dtype whose range encompasses the ranges of the supplied dtypes.\n",
    "    \"\"\"\n",
    "    best = DTypeInfo(base='bool', width=1)\n",
    "    for dtype in args:\n",
    "        current = DTYPE_INFOS[dtype]\n",
    "        if best.base != current.base:\n",
    "            if best.base == 'bool':\n",
    "                best = current\n",
    "            elif current.base == 'bool':\n",
    "                # Just use whatever we have so far; booleans can be coerced to anything.\n",
    "                pass\n",
    "            elif best.base == 'float' or current.base == 'float':\n",
    "                # We're unifying some integer type with a float.  The float needs to be\n",
    "                # at least twice the size of the integer type, clamped to float64.\n",
    "                best_width = best.width if best.base == 'float' else best.width * 2\n",
    "                current_width = current.width if current.base == 'float' else current.width * 2\n",
    "                width = max(best_width, current_width)\n",
    "                if width > 8:\n",
    "                    width = 8\n",
    "                best = DTypeInfo(base='float', width=width)\n",
    "            else:\n",
    "                # We're unifying an 'int' with a 'uint'.  The 'uint' can be held in an 'int' twice\n",
    "                # the width; if that pushes us up to width=16, use a float64.\n",
    "                best_width = best.width if best.base == 'int' else best.width * 2\n",
    "                current_width = current.width if current.base == 'int' else current.width * 2\n",
    "                width = max(best_width, current_width)\n",
    "                if width > 8:\n",
    "                    best = DTypeInfo(base='float', width=8)\n",
    "                else:\n",
    "                    best = DTypeInfo(base='int', width=width)\n",
    "\n",
    "    return INFO_DTYPES[best]\n",
    "\n",
    "\n",
    "def broadcast_dims(*args):\n",
    "    \"\"\"Computes the broadcast dimensions of the supplied dimensions.\n",
    "\n",
    "    Args:\n",
    "        args ([dims]): The list of dimension tuples to be broadcast.\n",
    "\n",
    "    Returns:\n",
    "        tuple(Value): The broadcasted dims tuple.\n",
    "    \"\"\"\n",
    "    dtuples = args\n",
    "    result_dcount = max((len(dtuple) for dtuple in dtuples))\n",
    "\n",
    "    def make_binding_broadcast(sizes):\n",
    "        \"\"\"Builds a bind-time Value for the broadcast of the supplied sizes.\n",
    "\n",
    "        Args:\n",
    "            sizes ([int or Value]): The sizes being broadcast.\n",
    "\n",
    "        Returns:\n",
    "            Value: The broadcasted size.\n",
    "        \"\"\"\n",
    "        vsizes = [sz for sz in sizes if isinstance(sz, Value)]\n",
    "        vsize_strs = ['I{}'.format(idx) for idx in range(len(vsizes))]\n",
    "        isize_strs = [str(sz) for sz in sizes if not isinstance(sz, Value)]\n",
    "        code = 'function ({var_sizes}) -> (O) {{ O = broadcast({sizes}); }}'.format(\n",
    "            var_sizes=', '.join(vsize_strs), sizes=', '.join(vsize_strs + isize_strs))\n",
    "        shape = Shape(plaidml.DType.UINT64, tuple())\n",
    "        operation = Operation(code, list(zip(vsize_strs, vsizes)), [('O', shape)])\n",
    "        return operation.outputs['O']\n",
    "\n",
    "    def make_axis(rdim_idx):\n",
    "        \"\"\"Builds a single axis of a broadcast output.\n",
    "\n",
    "        Args:\n",
    "            rdim_idx (int): The reversed index of the dimension within the broadcast output\n",
    "                            dimensions: zero corresponds to the last (greatest index) element\n",
    "                            of the tuple, result_dcount-1 corresponds to the first (0-index)\n",
    "                            element of the tuple.  (This is used because for dimension tuples,\n",
    "                            the 0-index corresponds to the greatest-stride element, while\n",
    "                            broadcasting aligns the smallest-stride elements).\n",
    "\n",
    "        Raises:\n",
    "            LogicError: If the dimensions are incompatible on this dimension.\n",
    "\n",
    "        Returns:\n",
    "            int or Value: The output dimension size.\n",
    "        \"\"\"\n",
    "        sizes = []\n",
    "        for dtuple in dtuples:\n",
    "            if len(dtuple) <= rdim_idx:\n",
    "                size = 1\n",
    "            else:\n",
    "                size = dtuple[len(dtuple) - rdim_idx - 1]\n",
    "            sizes.append(size)\n",
    "        sizes = [sz for sz in sizes if isinstance(sz, Value) or sz != 1]\n",
    "        if not sizes:\n",
    "            return 1\n",
    "        if len(sizes) == 1:\n",
    "            return sizes[0]\n",
    "        size = None\n",
    "        for this_size in sizes:\n",
    "            if isinstance(this_size, Value):\n",
    "                # This broadcast can only be computed at binding time.\n",
    "                return make_binding_broadcast(sizes)\n",
    "            if size and size != this_size:\n",
    "                raise LogicError(\n",
    "                    'Broadcast mismatch: {} and {} are incompatible; inputs were: {}'.format(\n",
    "                        this_size, size, ', '.join([\n",
    "                            '({})'.format(', '.join(str(dim) for dim in dtuple))\n",
    "                            for dtuple in dtuples\n",
    "                        ])))\n",
    "            size = this_size\n",
    "        return size\n",
    "\n",
    "    return tuple([make_axis(result_dcount - dim_idx - 1) for dim_idx in range(result_dcount)])\n",
    "\n",
    "\n",
    "def compute_aggregation_axes(dims, axes=None, keepdims=False):\n",
    "    \"\"\"Computes parameters for an aggregation-over-axes operation.\n",
    "\n",
    "    Args:\n",
    "        dims ([int or Value]): The dimensions of the value being aggregated.\n",
    "        axes ([int], optional): Defaults to None. The indices of the axes to aggregate over.\n",
    "        keepdims (bool, optional): Defaults to False. Iff true, keep the aggregated axes in the\n",
    "                                   output.\n",
    "\n",
    "    Returns:\n",
    "        (dims, axes, dict(string->string)): The resulting dimensions and axes, and a dictionary of\n",
    "                                            formatting replacements to use when building the TILE\n",
    "                                            operation.\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        axes = len(dims) - 1\n",
    "    if isinstance(axes, list) or isinstance(axes, tuple):\n",
    "        axes = [(len(dims) + i if i < 0 else i) for i in axes]\n",
    "    else:\n",
    "        if axes < 0:\n",
    "            axes = len(dims) + axes\n",
    "        axes = [axes]\n",
    "    axes.sort(reverse=True)\n",
    "    src_indices = ['x' + str(i) for i in range(len(dims))]\n",
    "    src_ranges = ['X' + str(i) for i in range(len(dims))]\n",
    "    dest_indices = src_indices[:]\n",
    "    dest_ranges = src_ranges[:]\n",
    "    dims = list(dims)\n",
    "    if keepdims:\n",
    "        for axis in axes:\n",
    "            dest_indices[axis] = 's' + dest_indices[axis]\n",
    "            dest_ranges[axis] = '1'\n",
    "            dims[axis] = 1\n",
    "    else:\n",
    "        for axis in axes:\n",
    "            del dest_indices[axis]\n",
    "            del dest_ranges[axis]\n",
    "            del dims[axis]\n",
    "\n",
    "    return tuple(dims), axes, {\n",
    "        'src_indices': ', '.join(src_indices),\n",
    "        'src_ranges': ', '.join(src_ranges),\n",
    "        'src_sep': ' : ' if src_indices else '',\n",
    "        'dest_indices': ', '.join(dest_indices),\n",
    "        'dest_ranges': ', '.join(dest_ranges),\n",
    "        'dest_sep': ' : ' if dest_indices else '',\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
